from fastapi import FastAPI, File, UploadFile, Form, HTTPException
from fastapi.responses import JSONResponse, FileResponse
import cv2
import numpy as np
import os
import torch
import torch.nn.functional as F
from torchvision import transforms
from PIL import Image
import io
import base64
from pymongo import MongoClient
import uuid
from datetime import datetime
import logging
import asyncio
import psutil
import gc
from typing import Optional, Tuple
import gdown
from huggingface_hub import hf_hub_download
import json
import albumentations as A
from concurrent.futures import ThreadPoolExecutor
import time
from scipy import ndimage
from skimage import segmentation, morphology
from scipy.spatial.distance import cdist

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

app = FastAPI(title="Professional Background Removal API", version="6.0")

# Global configuration
UPLOAD_DIR = os.getenv("UPLOAD_DIR", "uploads")
RESULT_DIR = os.getenv("RESULT_DIR", "results")
MODEL_CACHE_DIR = os.getenv("MODEL_CACHE_DIR", "models/pretrained")

# Create directories
for dir_path in [UPLOAD_DIR, RESULT_DIR, MODEL_CACHE_DIR]:
    os.makedirs(dir_path, exist_ok=True)

# MongoDB connection
client = None
db = None
collection = None

try:
    mongodb_uri = os.getenv("MONGODB_URI", "mongodb://localhost:27017/")
    client = MongoClient(mongodb_uri, serverSelectionTimeoutMS=2000)
    db = client["segmentation_db"]
    collection = db["images"]
    client.admin.command('ping')
    logger.info("âœ“ MongoDB connected")
except:
    logger.warning("âœ— MongoDB not available - continuing without database")
    client = None

# Check device availability
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
logger.info(f"ðŸ–¥ï¸  Device: {device}")

# Device-specific optimizations
if device.type == "cuda":
    # GPU optimizations
    torch.backends.cudnn.benchmark = True  # Optimize for consistent input sizes
    torch.backends.cudnn.deterministic = False  # Allow non-deterministic algorithms for speed
    logger.info(f"ðŸš€ GPU optimizations enabled for {torch.cuda.get_device_name(0)}")
    logger.info(f"ðŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
else:
    # CPU optimizations
    torch.set_num_threads(4)  # Optimize for your CPU cores
    torch.set_num_interop_threads(4)
    logger.info("ðŸ”§ CPU optimizations applied")

# Thread pool for CPU-intensive operations  
max_workers = 1 if device.type == "cuda" else 2  # GPU handles parallelism internally
executor = ThreadPoolExecutor(max_workers=max_workers)


class ModelManager:
    """Manages multiple background removal models"""
    
    def __init__(self):
        self.models = {}
        self.model_configs = {
            "modnet": {
                "url": "https://drive.google.com/uc?id=1mcr7ALciuAsHCpLnrtG_eop5-EYhbCmz",
                "filename": "modnet_photographic_portrait_matting.ckpt",
                "input_size": (512, 512),
                "description": "MODNet - High quality portrait matting"
            }
            # Removed U2Net due to download issues - MODNet + OpenCV fallback is sufficient
        }
        
    async def download_models(self):
        """Download all models if not present"""
        for model_name, config in self.model_configs.items():
            model_path = os.path.join(MODEL_CACHE_DIR, config["filename"])
            
            if not os.path.exists(model_path):
                logger.info(f"ðŸ“¥ Downloading {model_name}...")
                try:
                    if "url" in config:
                        # Direct URL download
                        if "drive.google.com" in config["url"]:
                            gdown.download(config["url"], model_path, quiet=False)
                        else:
                            # Direct download for other URLs
                            import requests
                            response = requests.get(config["url"], stream=True)
                            response.raise_for_status()
                            with open(model_path, 'wb') as f:
                                for chunk in response.iter_content(chunk_size=8192):
                                    f.write(chunk)
                    elif "hf_repo" in config:
                        # Hugging Face download
                        hf_hub_download(
                            repo_id=config["hf_repo"],
                            filename=config["filename"],
                            cache_dir=MODEL_CACHE_DIR
                        )
                    logger.info(f"âœ“ {model_name} downloaded")
                except Exception as e:
                    logger.error(f"âœ— Failed to download {model_name}: {e}")
            else:
                logger.info(f"âœ“ {model_name} already exists")

    def load_modnet(self) -> Optional[torch.nn.Module]:
        """Load MODNet model"""
        try:
            import torch.nn as nn
            
            class MODNet(nn.Module):
                def __init__(self):
                    super(MODNet, self).__init__()
                    # Simplified MODNet architecture
                    # In production, use the full MODNet implementation
                    self.encoder = nn.Sequential(
                        nn.Conv2d(3, 32, 3, padding=1),
                        nn.ReLU(),
                        nn.Conv2d(32, 64, 3, padding=1),
                        nn.ReLU(),
                        nn.MaxPool2d(2),
                        nn.Conv2d(64, 128, 3, padding=1),
                        nn.ReLU(),
                        nn.Conv2d(128, 256, 3, padding=1),
                        nn.ReLU(),
                        nn.MaxPool2d(2)
                    )
                    
                    self.decoder = nn.Sequential(
                        nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),
                        nn.Conv2d(256, 128, 3, padding=1),
                        nn.ReLU(),
                        nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),
                        nn.Conv2d(128, 64, 3, padding=1),
                        nn.ReLU(),
                        nn.Conv2d(64, 1, 3, padding=1),
                        nn.Sigmoid()
                    )
                    
                def forward(self, x):
                    x = self.encoder(x)
                    x = self.decoder(x)
                    return x
            
            model = MODNet().to(device)
            model_path = os.path.join(MODEL_CACHE_DIR, "modnet_photographic_portrait_matting.ckpt")
            
            if os.path.exists(model_path):
                try:
                    checkpoint = torch.load(model_path, map_location=device, weights_only=False)
                    model.load_state_dict(checkpoint, strict=False)
                    logger.info("âœ“ MODNet loaded with pretrained weights")
                except:
                    logger.warning("âš ï¸  MODNet loaded without pretrained weights")
            
            model.eval()
            return model
        except Exception as e:
            logger.error(f"âœ— Failed to load MODNet: {e}")
            return None

    def load_u2net(self) -> Optional[torch.nn.Module]:
        """Load UÂ²-Net model"""
        try:
            import torch.nn as nn
            
            class U2NET(nn.Module):
                def __init__(self):
                    super(U2NET, self).__init__()
                    # Simplified UÂ²-Net architecture
                    # In production, use the full UÂ²-Net implementation
                    self.encoder1 = nn.Sequential(
                        nn.Conv2d(3, 64, 3, padding=1),
                        nn.ReLU(),
                        nn.Conv2d(64, 64, 3, padding=1),
                        nn.ReLU()
                    )
                    
                    self.encoder2 = nn.Sequential(
                        nn.MaxPool2d(2),
                        nn.Conv2d(64, 128, 3, padding=1),
                        nn.ReLU(),
                        nn.Conv2d(128, 128, 3, padding=1),
                        nn.ReLU()
                    )
                    
                    self.encoder3 = nn.Sequential(
                        nn.MaxPool2d(2),
                        nn.Conv2d(128, 256, 3, padding=1),
                        nn.ReLU(),
                        nn.Conv2d(256, 256, 3, padding=1),
                        nn.ReLU()
                    )
                    
                    self.decoder = nn.Sequential(
                        nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),
                        nn.Conv2d(256, 128, 3, padding=1),
                        nn.ReLU(),
                        nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),
                        nn.Conv2d(128, 64, 3, padding=1),
                        nn.ReLU(),
                        nn.Conv2d(64, 1, 3, padding=1),
                        nn.Sigmoid()
                    )
                    
                def forward(self, x):
                    e1 = self.encoder1(x)
                    e2 = self.encoder2(e1)
                    e3 = self.encoder3(e2)
                    
                    d = self.decoder(e3)
                    return d
            
            model = U2NET().to(device)
            model_path = os.path.join(MODEL_CACHE_DIR, "u2net.pth")
            
            if os.path.exists(model_path):
                try:
                    model.load_state_dict(torch.load(model_path, map_location=device, weights_only=False), strict=False)
                    logger.info("âœ“ UÂ²-Net loaded with pretrained weights")
                except:
                    logger.warning("âš ï¸  UÂ²-Net loaded without pretrained weights")
            
            model.eval()
            return model
        except Exception as e:
            logger.error(f"âœ— Failed to load UÂ²-Net: {e}")
            return None

    def load_models(self):
        """Load all available models"""
        self.models["modnet"] = self.load_modnet()
        # Removed U2Net loading due to download issues
        
        # Filter out None models
        self.models = {k: v for k, v in self.models.items() if v is not None}
        logger.info(f"âœ“ Loaded {len(self.models)} models: {list(self.models.keys())}")
        
        if len(self.models) == 0:
            logger.warning("âš ï¸  No deep learning models loaded - will use OpenCV fallback only")


class AdvancedBackgroundRemover:
    """Advanced background removal using multiple deep learning models"""
    
    def __init__(self, model_manager: ModelManager):
        self.model_manager = model_manager
        self.preprocessing = A.Compose([
            A.LongestMaxSize(max_size=1024),
            A.PadIfNeeded(min_height=512, min_width=512, border_mode=cv2.BORDER_CONSTANT),
        ])
        
    def preprocess_image(self, image: np.ndarray, target_size: Tuple[int, int]) -> torch.Tensor:
        """Preprocess image for model input"""
        # Convert BGR to RGB
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # Resize
        image_resized = cv2.resize(image_rgb, target_size)
        
        # Normalize
        image_normalized = image_resized.astype(np.float32) / 255.0
        
        # Convert to tensor
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        image_tensor = transform(image_normalized).unsqueeze(0).to(device)
        return image_tensor

    def postprocess_mask(self, mask: torch.Tensor, original_shape: Tuple[int, int]) -> np.ndarray:
        """Postprocess model output to create refined mask"""
        # Convert to numpy
        mask_np = mask.squeeze().cpu().detach().numpy()
        
        # Resize to original dimensions
        mask_resized = cv2.resize(mask_np, (original_shape[1], original_shape[0]))
        
        # Apply morphological operations for refinement
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
        mask_refined = cv2.morphologyEx(mask_resized, cv2.MORPH_CLOSE, kernel, iterations=2)
        mask_refined = cv2.morphologyEx(mask_refined, cv2.MORPH_OPEN, kernel, iterations=1)
        
        # Gaussian blur for edge smoothing
        mask_smooth = cv2.GaussianBlur(mask_refined, (5, 5), 0)
        
        return mask_smooth

    def create_trimap(self, mask: np.ndarray, erode_ksize: int = 10, dilate_ksize: int = 10) -> np.ndarray:
        """Create trimap for better edge handling"""
        kernel_erode = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (erode_ksize, erode_ksize))
        kernel_dilate = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (dilate_ksize, dilate_ksize))
        
        eroded = cv2.erode(mask, kernel_erode, iterations=1)
        dilated = cv2.dilate(mask, kernel_dilate, iterations=1)
        
        trimap = np.zeros_like(mask)
        trimap[eroded > 0.5] = 1  # Foreground
        trimap[dilated < 0.5] = 0  # Background
        trimap[(dilated >= 0.5) & (eroded <= 0.5)] = 0.5  # Unknown
        
        return trimap

    def apply_guided_filter(self, image: np.ndarray, mask: np.ndarray, radius: int = 8, eps: float = 0.2) -> np.ndarray:
        """Apply guided filter for better edge preservation"""
        try:
            # Make sure opencv-contrib-python is installed for ximgproc
            guided_mask = cv2.ximgproc.guidedFilter(
                guide=image.astype(np.uint8),
                src=(mask * 255).astype(np.uint8),
                radius=radius,
                eps=eps
            ) / 255.0
            return guided_mask
        except:
            # Fallback to gaussian blur if guided filter not available
            logger.warning("Guided filter not available, using Gaussian blur fallback")
            return cv2.GaussianBlur(mask, (radius*2+1, radius*2+1), 0)

    def create_trimap_matting(self, mask: np.ndarray, erode_size: int = 10, dilate_size: int = 20) -> np.ndarray:
        """Create trimap for alpha matting"""
        # Create sure foreground (eroded mask)
        kernel_erode = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (erode_size, erode_size))
        sure_fg = cv2.erode(mask, kernel_erode, iterations=1)
        
        # Create sure background (dilated inverse mask)
        kernel_dilate = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (dilate_size, dilate_size))
        sure_bg = cv2.dilate(mask, kernel_dilate, iterations=1)
        
        # Create trimap
        trimap = np.zeros_like(mask, dtype=np.uint8)
        trimap[sure_fg > 0.5] = 255  # Foreground
        trimap[sure_bg < 0.5] = 0    # Background
        trimap[(sure_bg >= 0.5) & (sure_fg <= 0.5)] = 128  # Unknown
        
        return trimap

    def simple_alpha_matting(self, image: np.ndarray, trimap: np.ndarray) -> np.ndarray:
        """Simple alpha matting implementation"""
        alpha = trimap.astype(np.float32) / 255.0
        
        # For unknown regions, use color-based estimation
        unknown_mask = (trimap == 128)
        
        if np.any(unknown_mask):
            # Convert to LAB for better color distance
            lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB).astype(np.float32)
            
            # Get foreground and background samples
            fg_mask = (trimap == 255)
            bg_mask = (trimap == 0)
            
            if np.any(fg_mask) and np.any(bg_mask):
                # Sample colors from known regions
                fg_samples = lab_image[fg_mask]
                bg_samples = lab_image[bg_mask]
                
                # For each unknown pixel, estimate alpha
                unknown_pixels = lab_image[unknown_mask]
                
                if len(fg_samples) > 0 and len(bg_samples) > 0:
                    # Calculate distances to nearest foreground and background
                    fg_mean = np.mean(fg_samples, axis=0)
                    bg_mean = np.mean(bg_samples, axis=0)
                    
                    # Distance-based alpha estimation
                    fg_distances = np.linalg.norm(unknown_pixels - fg_mean, axis=1)
                    bg_distances = np.linalg.norm(unknown_pixels - bg_mean, axis=1)
                    
                    # Calculate alpha based on relative distances
                    total_distances = fg_distances + bg_distances
                    alpha_values = np.where(total_distances > 0, 
                                          bg_distances / total_distances, 
                                          0.5)
                    
                    # Apply calculated alphas to unknown regions
                    alpha[unknown_mask] = alpha_values
        
        return alpha

    def expand_mask_for_accessories(self, image: np.ndarray, mask: np.ndarray, max_expansion: int = 50) -> np.ndarray:
        """Expand mask to include accessories using smart techniques"""
        # Convert to binary
        binary_mask = (mask > 0.5).astype(np.uint8)
        
        # Find main contour
        contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        if not contours:
            return mask
        
        largest_contour = max(contours, key=cv2.contourArea)
        
        # Strategy 1: Convex hull expansion
        hull = cv2.convexHull(largest_contour)
        hull_mask = np.zeros_like(binary_mask)
        cv2.fillPoly(hull_mask, [hull], 1)
        
        # Strategy 2: Smart morphological expansion
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (max_expansion, max_expansion))
        dilated_mask = cv2.dilate(binary_mask, kernel, iterations=1)
        
        # Strategy 3: Color-guided expansion
        # Analyze colors in the existing mask
        masked_area = image[binary_mask > 0]
        if len(masked_area) > 0:
            mean_color = np.mean(masked_area, axis=0)
            std_color = np.std(masked_area, axis=0) + 1e-6
            
            # Create color similarity mask
            color_diff = np.abs(image.astype(np.float32) - mean_color)
            color_distance = np.sqrt(np.sum((color_diff / std_color) ** 2, axis=2))
            color_mask = (color_distance < 2.0).astype(np.uint8)  # 2 standard deviations
            
            # Combine expansion strategies safely
            # Only expand where color is similar and within dilated region
            safe_expansion = np.logical_and(
                np.logical_and(dilated_mask, color_mask),
                hull_mask
            ).astype(np.uint8)
            
            # Ensure we don't lose the original mask
            final_mask = np.maximum(binary_mask, safe_expansion)
        else:
            # Fallback to conservative expansion
            final_mask = hull_mask
        
        return final_mask.astype(np.float32)

    def enhance_mask_edges(self, image: np.ndarray, mask: np.ndarray) -> np.ndarray:
        """Enhance mask edges using guided filtering and edge refinement"""
        # Apply guided filter first
        guided_mask = self.apply_guided_filter(image, mask, radius=8, eps=0.2)
        
        # Create trimap for matting
        trimap = self.create_trimap_matting(guided_mask, erode_size=5, dilate_size=15)
        
        # Apply simple alpha matting
        alpha = self.simple_alpha_matting(image, trimap)
        
        # Post-process alpha for smoothness
        alpha = self.post_process_alpha(alpha)
        
        return alpha

    def post_process_alpha(self, alpha: np.ndarray) -> np.ndarray:
        """Post-process alpha channel for quality"""
        # Convert to uint8 for morphological operations
        alpha_uint8 = (alpha * 255).astype(np.uint8)
        
        # Fill small holes
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
        alpha_closed = cv2.morphologyEx(alpha_uint8, cv2.MORPH_CLOSE, kernel, iterations=2)
        
        # Smooth edges
        alpha_smooth = cv2.medianBlur(alpha_closed, 5)
        alpha_smooth = cv2.GaussianBlur(alpha_smooth, (3, 3), 0.5)
        
        # Apply edge enhancement
        # Find edge areas
        edges = cv2.Canny(alpha_smooth, 50, 150)
        edge_kernel = np.ones((3, 3), np.uint8)
        edge_areas = cv2.dilate(edges, edge_kernel, iterations=2)
        
        # Apply slight sharpening to edge areas
        kernel_sharpen = np.array([[-0.5, -1, -0.5], [-1, 7, -1], [-0.5, -1, -0.5]])
        alpha_sharpened = cv2.filter2D(alpha_smooth, -1, kernel_sharpen)
        
        # Combine original and sharpened based on edge areas
        alpha_final = alpha_smooth.copy()
        alpha_final[edge_areas > 0] = alpha_sharpened[edge_areas > 0]
        
        # Convert back to float and clamp
        return np.clip(alpha_final.astype(np.float32) / 255.0, 0, 1)

    async def remove_background_deep(self, image: np.ndarray) -> Tuple[np.ndarray, str, float]:
        """Remove background using deep learning models (if available)"""
        start_time = time.time()
        
        original_height, original_width = image.shape[:2]
        best_mask = None
        best_model = "none"
        best_confidence = 0.0
        
        # Only try deep learning if models are loaded
        if not self.model_manager.models:
            logger.info("No deep learning models available, skipping...")
            raise Exception("No deep learning models loaded")
        
        # Try each model and select the best result
        for model_name, model in self.model_manager.models.items():
            try:
                logger.info(f"ðŸ”„ Trying {model_name}...")
                
                # Get model configuration
                config = self.model_manager.model_configs[model_name]
                target_size = config["input_size"]
                
                # Preprocess
                input_tensor = self.preprocess_image(image, target_size)
                
                # Inference
                with torch.no_grad():
                    output = model(input_tensor)
                    
                # Postprocess
                mask = self.postprocess_mask(output, (original_height, original_width))
                
                # Calculate confidence (coverage and edge quality)
                coverage = np.mean(mask)
                edges = cv2.Canny((mask * 255).astype(np.uint8), 50, 150)
                edge_quality = np.mean(edges) / 255.0
                confidence = coverage * 0.7 + edge_quality * 0.3
                
                logger.info(f"  {model_name}: coverage={coverage:.3f}, edge_quality={edge_quality:.3f}, confidence={confidence:.3f}")
                
                # Only accept if decent confidence and coverage
                if confidence > 0.3 and coverage > 0.1:
                    if confidence > best_confidence:
                        best_mask = mask
                        best_model = model_name
                        best_confidence = confidence
                    
            except Exception as e:
                logger.error(f"âœ— {model_name} failed: {e}")
                continue
        
        processing_time = time.time() - start_time
        
        if best_mask is None or best_confidence < 0.3:
            raise Exception(f"Deep learning models failed (best confidence: {best_confidence:.3f})")
            
        logger.info(f"âœ“ Best model: {best_model} (confidence: {best_confidence:.3f})")
        return best_mask, best_model, processing_time

    def apply_advanced_matting(self, image: np.ndarray, mask: np.ndarray) -> np.ndarray:
        """Apply advanced matting techniques for better edges"""
        # Enhance mask edges first
        enhanced_alpha = self.enhance_mask_edges(image, mask)
        return enhanced_alpha

    def create_professional_output(self, image: np.ndarray, alpha: np.ndarray) -> np.ndarray:
        """Create professional output with transparency"""
        # Ensure alpha is in the right range
        alpha = np.clip(alpha, 0, 1)
        
        # Create RGBA image
        height, width = image.shape[:2]
        rgba_image = np.zeros((height, width, 4), dtype=np.uint8)
        
        # Copy RGB channels
        rgba_image[:, :, :3] = image
        
        # Set alpha channel
        rgba_image[:, :, 3] = (alpha * 255).astype(np.uint8)
        
        return rgba_image

    async def remove_background(self, image: np.ndarray) -> Tuple[np.ndarray, str, float]:
        """Main background removal function - Enhanced OpenCV + Optional DL"""
        try:
            # Always start with enhanced OpenCV methods
            result, method_used, processing_time = await self.remove_background_opencv_enhanced(image)
            
            # If deep learning models available and OpenCV result is not optimal, try enhancement
            if self.model_manager.models and "fallback" not in method_used.lower():
                try:
                    # Try deep learning for comparison  
                    mask, dl_model, dl_time = await self.remove_background_deep(image)
                    
                    # Enhance the deep learning mask with our advanced techniques
                    enhanced_alpha = self.apply_advanced_matting(image, mask)
                    dl_result = self.create_professional_output(image, enhanced_alpha)
                    
                    # Compare results
                    dl_coverage = np.mean(enhanced_alpha)
                    
                    # Check if result has alpha channel (RGBA) or is RGB
                    if result.shape[2] == 4:  # RGBA result
                        opencv_mask = result[:, :, 3] / 255.0
                    else:  # RGB result with white background
                        opencv_mask = np.where(np.all(result == [255, 255, 255], axis=2), 0, 1)
                    
                    opencv_coverage = np.mean(opencv_mask)
                    
                    if dl_coverage > opencv_coverage * 1.2:  # 20% better
                        logger.info(f"Using enhanced deep learning ({dl_coverage:.3f} vs {opencv_coverage:.3f})")
                        result = dl_result
                        method_used = f"Enhanced {dl_model} + Advanced Matting"
                        processing_time = dl_time
                    else:
                        logger.info(f"Enhanced OpenCV better ({opencv_coverage:.3f} vs {dl_coverage:.3f})")
                        
                except Exception as e:
                    logger.info(f"Deep learning enhancement failed: {e}, using enhanced OpenCV result")
            
            return result, method_used, processing_time
            
        except Exception as e:
            logger.error(f"All background removal methods failed: {e}")
            
            # Ultimate fallback - return original with warning
            return image, "No processing (failed)", 0.0

    async def remove_background_opencv_enhanced(self, image: np.ndarray) -> Tuple[np.ndarray, str, float]:
        """ENHANCED OpenCV methods with better edge handling and accessory detection"""
        start_time = time.time()
        
        logger.info("ðŸ”„ Using ENHANCED OpenCV methods...")
        
        height, width = image.shape[:2]
        best_mask = None
        best_method = "unknown"
        best_coverage = 0.0
        
        # Method 1: Enhanced Mask R-CNN
        try:
            logger.info("Trying Enhanced Mask R-CNN...")
            
            # Import required libraries for Mask R-CNN
            import torchvision.transforms as T
            from torchvision.models.detection import maskrcnn_resnet50_fpn_v2
            
            # Load pre-trained Mask R-CNN
            maskrcnn_model = maskrcnn_resnet50_fpn_v2(pretrained=True).to(device)
            maskrcnn_model.eval()
            
            # Preprocess image for Mask R-CNN
            transform = T.Compose([T.ToTensor()])
            img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            input_tensor = transform(img_rgb).unsqueeze(0).to(device)
            
            # Run inference
            with torch.no_grad():
                predictions = maskrcnn_model(input_tensor)
            
            # Extract masks for people and relevant objects
            # Expanded class list for better detection
            relevant_classes = [1, 16, 17, 18, 19, 20, 21, 84, 85, 86, 87, 88, 89, 90]
            
            pred = predictions[0]
            scores = pred['scores'].cpu().numpy()
            labels = pred['labels'].cpu().numpy()
            masks = pred['masks'].cpu().numpy()
            
            # Combine masks with lower threshold for better detection
            combined_mask = np.zeros((height, width), dtype=np.float32)
            
            for i, (score, label, mask) in enumerate(zip(scores, labels, masks)):
                if score > 0.3 and label in relevant_classes:  # Lowered threshold
                    mask_resized = cv2.resize(mask[0], (width, height))
                    combined_mask = np.maximum(combined_mask, mask_resized)
                    logger.info(f"  Found class {label} with confidence {score:.3f}")
            
            # Enhanced post-processing
            if np.max(combined_mask) > 0:
                # Convert to binary
                rcnn_mask = (combined_mask > 0.3).astype(np.uint8)
                
                # Expand for accessories
                rcnn_mask = self.expand_mask_for_accessories(image, rcnn_mask, max_expansion=40)
                
                # Apply edge enhancement
                rcnn_alpha = self.enhance_mask_edges(image, rcnn_mask)
                
                coverage = np.mean(rcnn_alpha)
                logger.info(f"Enhanced Mask R-CNN coverage: {coverage:.3f}")
                
                if coverage > best_coverage and coverage > 0.05:
                    best_mask = rcnn_alpha
                    best_method = "Enhanced Mask R-CNN"
                    best_coverage = coverage
            
        except Exception as e:
            logger.error(f"Enhanced Mask R-CNN failed: {e}")
        
        # Method 2: Enhanced GrabCut (if Mask R-CNN didn't work well)
        if best_coverage < 0.3:
            try:
                logger.info("Trying enhanced GrabCut...")
                
                # Multiple GrabCut attempts with different rectangles
                margin_x = max(1, int(width * 0.002))
                margin_y = max(1, int(height * 0.002))
                rect1 = (margin_x, margin_y, width - 2*margin_x, height - 2*margin_y)
                
                mask1 = np.zeros((height, width), np.uint8)
                bgdModel1 = np.zeros((1, 65), np.float64)
                fgdModel1 = np.zeros((1, 65), np.float64)
                
                cv2.grabCut(image, mask1, rect1, bgdModel1, fgdModel1, 10, cv2.GC_INIT_WITH_RECT)
                result_mask1 = np.where((mask1 == 2) | (mask1 == 0), 0, 1).astype('uint8')
                
                # Expand for accessories
                expanded_mask = self.expand_mask_for_accessories(image, result_mask1, max_expansion=50)
                
                # Apply enhanced edge processing
                enhanced_alpha = self.enhance_mask_edges(image, expanded_mask)
                
                coverage = np.mean(enhanced_alpha)
                logger.info(f"Enhanced GrabCut coverage: {coverage:.3f}")
                
                if coverage > 0.1 and coverage < 0.8:
                    if coverage > best_coverage:
                        best_mask = enhanced_alpha
                        best_method = "Enhanced GrabCut"
                        best_coverage = coverage
                
            except Exception as e:
                logger.error(f"Enhanced GrabCut failed: {e}")
        
        # Method 3: Color clustering with enhancement (if others didn't work well)
        if best_coverage < 0.2:
            try:
                logger.info("Trying enhanced color clustering...")
                
                # Convert to different color spaces
                hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
                lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
                
                # Sample from center region
                center_h, center_w = height//2, width//2
                sample_size = min(height, width) // 6
                
                center_region_bgr = image[center_h-sample_size:center_h+sample_size, 
                                        center_w-sample_size:center_w+sample_size]
                center_region_hsv = hsv[center_h-sample_size:center_h+sample_size,
                                        center_w-sample_size:center_w+sample_size]
                center_region_lab = lab[center_h-sample_size:center_h+sample_size,
                                        center_w-sample_size:center_w+sample_size]
                
                if center_region_bgr.size > 0:
                    # Calculate mean colors
                    mean_bgr = np.mean(center_region_bgr.reshape(-1, 3), axis=0)
                    mean_hsv = np.mean(center_region_hsv.reshape(-1, 3), axis=0)
                    mean_lab = np.mean(center_region_lab.reshape(-1, 3), axis=0)
                    
                    # Create color masks with generous ranges
                    bgr_lower = np.array([max(0, mean_bgr[0]-60), max(0, mean_bgr[1]-60), max(0, mean_bgr[2]-60)])
                    bgr_upper = np.array([min(255, mean_bgr[0]+60), min(255, mean_bgr[1]+60), min(255, mean_bgr[2]+60)])
                    bgr_mask = cv2.inRange(image, bgr_lower, bgr_upper)
                    
                    # Combine masks
                    color_mask = (bgr_mask // 255).astype(np.uint8)
                    
                    # Apply enhancements
                    enhanced_mask = self.expand_mask_for_accessories(image, color_mask, max_expansion=40)
                    enhanced_alpha = self.enhance_mask_edges(image, enhanced_mask)
                    
                    coverage = np.mean(enhanced_alpha)
                    logger.info(f"Enhanced color clustering coverage: {coverage:.3f}")
                    
                    if coverage > 0.1 and coverage < 0.8:
                        if coverage > best_coverage:
                            best_mask = enhanced_alpha
                            best_method = "Enhanced Color Clustering"
                            best_coverage = coverage
                        
            except Exception as e:
                logger.error(f"Enhanced color clustering failed: {e}")
        
        # Fallback: Maximum ellipse with enhancement
        if best_mask is None or best_coverage < 0.05:
            logger.warning("Using enhanced ellipse fallback")
            fallback_mask = np.zeros((height, width), np.uint8)
            cv2.ellipse(fallback_mask, (width//2, height//2), 
                       (int(width*0.47), int(height*0.47)), 0, 0, 360, 1, -1)
            
            # Even enhance the fallback
            best_mask = self.enhance_mask_edges(image, fallback_mask)
            best_method = "Enhanced ellipse fallback"
            best_coverage = np.mean(best_mask)
        
        processing_time = time.time() - start_time
        
        # Create professional RGBA output
        if best_mask is not None:
            result = self.create_professional_output(image, best_mask)
        else:
            # Emergency fallback to white background
            result = image.copy()
            result[best_mask == 0] = [255, 255, 255]
        
        logger.info(f"âœ“ Best enhanced method: {best_method}, coverage: {best_coverage:.3f}")
        
        return result, f"{best_method} (Enhanced)", processing_time


# Initialize components
model_manager = ModelManager()
bg_remover = None

# Use lifespan instead of deprecated on_event
from contextlib import asynccontextmanager

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    global bg_remover
    
    logger.info("ðŸš€ Starting Professional Background Removal API...")
    
    # Download models
    await model_manager.download_models()
    
    # Load models
    model_manager.load_models()
    
    # Initialize background remover
    bg_remover = AdvancedBackgroundRemover(model_manager)
    
    logger.info("âœ“ API ready for processing!")
    
    yield
    
    # Shutdown
    logger.info("ðŸ›‘ Shutting down...")

app = FastAPI(title="Professional Background Removal API", version="6.0", lifespan=lifespan)

@app.get("/")
def read_root():
    return {
        "message": "Professional Background Removal API",
        "version": "6.0 - State-of-the-Art",
        "models_loaded": list(model_manager.models.keys()) if model_manager.models else [],
        "device": str(device),
        "features": [
            "MODNet support",
            "UÂ²-Net support",
            "Advanced matting",
            "Edge refinement",
            "Multiple model ensemble",
            "Professional quality output"
        ]
    }

@app.get("/health")
def health_check():
    """Enhanced health check with system metrics"""
    memory_info = psutil.virtual_memory()
    
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "models_available": list(model_manager.models.keys()),
        "device": str(device),
        "system": {
            "memory_usage": f"{memory_info.percent}%",
            "memory_available": f"{memory_info.available / (1024**3):.1f}GB",
            "cpu_count": psutil.cpu_count()
        },
        "torch_info": {
            "version": torch.__version__,
            "cuda_available": torch.cuda.is_available(),
            "cuda_device_count": torch.cuda.device_count() if torch.cuda.is_available() else 0
        }
    }

@app.post("/segmentation")
async def segment_image(challenge: str = Form(...), input: UploadFile = File(...)):
    """Professional background removal endpoint"""
    start_time = datetime.now()
    
    try:
        logger.info(f"=== NEW PROFESSIONAL REQUEST ===")
        logger.info(f"Challenge: {challenge}, File: {input.filename}")
        
        if challenge != "cv3":
            return JSONResponse(
                status_code=400,
                content={"message": f"Only cv3 supported, received: {challenge}"}
            )
        
        if not input.content_type.startswith("image/"):
            return JSONResponse(
                status_code=400,
                content={"message": "File must be an image"}
            )
        
        # Save uploaded file
        file_id = str(uuid.uuid4())
        file_extension = os.path.splitext(input.filename)[1] or ".jpg"
        file_path = os.path.join(UPLOAD_DIR, f"{file_id}{file_extension}")
        
        contents = await input.read()
        with open(file_path, "wb") as f:
            f.write(contents)
        
        logger.info(f"File saved: {len(contents)} bytes")
        
        # Read and process image
        image = cv2.imread(file_path)
        if image is None:
            raise HTTPException(status_code=400, detail="Could not read image")
        
        original_height, original_width = image.shape[:2]
        logger.info(f"Processing: {original_width}x{original_height}")
        
        # Remove background
        result, method_used, processing_time_seconds = await bg_remover.remove_background(image)
        
        # Save result
        result_path = os.path.join(RESULT_DIR, f"{file_id}_professional{file_extension}")
        
        # Save as PNG to preserve transparency if available
        if result.shape[2] == 4:  # RGBA
            result_path = result_path.replace(file_extension, ".png")
            cv2.imwrite(result_path, result)
        else:  # RGB
            cv2.imwrite(result_path, result)
        
        # Memory cleanup
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            logger.info(f"ðŸ§¹ GPU memory cleaned. Allocated: {torch.cuda.memory_allocated(0) / 1024**2:.1f}MB")
        
        # Store in MongoDB if available
        if collection is not None:
            try:
                total_time = (datetime.now() - start_time).total_seconds()
                document = {
                    "original_file": file_path,
                    "processed_file": result_path,
                    "challenge": challenge,
                    "timestamp": start_time,
                    "file_id": file_id,
                    "original_size": f"{original_width}x{original_height}",
                    "method_used": method_used,
                    "processing_time_seconds": processing_time_seconds,
                    "total_time_seconds": total_time,
                    "device": str(device),
                    "version": "Professional v6.0"
                }
                collection.insert_one(document)
                logger.info("Document stored in MongoDB")
            except Exception as e:
                logger.warning(f"MongoDB storage failed: {e}")
        
        total_time = (datetime.now() - start_time).total_seconds()
        logger.info(f"=== PROCESSING COMPLETED in {total_time:.2f}s ===")
        logger.info(f"Method: {method_used}, Processing: {processing_time_seconds:.2f}s")
        
        return JSONResponse(content={
            "message": "succeed", 
            "file_id": file_id,
            "method_used": method_used,
            "processing_time_seconds": processing_time_seconds,
            "total_time_seconds": total_time,
            "models_available": list(model_manager.models.keys()),
            "version": "Professional v6.0"
        })
        
    except Exception as e:
        logger.error(f"Processing error: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={"message": f"Processing failed: {str(e)}"}
        )

@app.get("/result/{file_id}")
async def get_result(file_id: str):
    """Get processing result"""
    # Check for different extensions
    for ext in ['.png', '.jpg', '.jpeg']:
        result_path = os.path.join(RESULT_DIR, f"{file_id}_professional{ext}")
        if os.path.exists(result_path):
            return FileResponse(result_path)
    
    return JSONResponse(
        status_code=404,
        content={"message": "Result not found"}
    )

@app.get("/models")
def get_models():
    """Get information about loaded models"""
    return {
        "loaded_models": list(model_manager.models.keys()),
        "model_configs": model_manager.model_configs,
        "device": str(device)
    }

if __name__ == "__main__":
    import uvicorn
    logger.info("ðŸš€ Starting Professional Background Removal API")
    logger.info(f"ðŸ§  Device: {device}")
    logger.info(f"ðŸ”¥ PyTorch version: {torch.__version__}")
    uvicorn.run(app, host="0.0.0.0", port=8080)